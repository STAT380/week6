---
title: "Week 5"
format: html
editor: visual
---
# Weekly 5 

1. Multicollinearity
2. Variable selection
3. Shrinkage Estimators

Packages we will require this week

```{r}


packages <- c(
    "ISLR2",
    "dplyr",
    "tidyr",
    "readr",
    "purrr",
    "glmnet",
    "caret",
    "car",
    "corrplot",
    "repr",
    "glmnet",
    "caret",
    "repr",
    "torch",
    "mlbench"
)

#renv::install(packages)
sapply(packages, require, character.only=TRUE)


library(ISLR2)
library(dplyr)
```

```{r}
library(tidyr)
library(readr)
library(purrr)
library(glmnet)
library(caret)
library(car)
# renv::install(packages)
# lapply(packages, require, character.only = TRUE)
```
In this class, we are going to look at variable selection. Consider theBoston housing dataset
which is described here:

```{r}
library(ISLR2)
attach(Boston)
```

```{r}
df <- Boston
head(df)
```

The attach() function in R is used to make an object part of the search path. In this case,
Boston is a built-in data frame in R, containing data on housing values in suburbs of Boston.
The attach(Boston) function makes this data frame part of the search path, allowing the user
to refer to its variables by name (e.g., medv, rm, nox,rad) without having to type the data
frame name Boston$ before each variable name.


**Explanation of the variables**

The original data are 506 observations on 14 variables, medv being the target variable:

- crimper capita crime rate by town
- znproportion of residential land zoned for lots over 25,000sq.ft
- indusproportion of non-retail business acres per town.
- chasCharles River dummy variable( =1 if tract bounds river; 0 otherwise)
- noxnitric oxides concentration (parts per 10 million)
- rmaverage number of rooms per dwelling
- ageproportion of owner-occupied units built prior to 1940
- disweighted distance to five Boston employment centres
- radindex of accessibility to radial highways
- taxfull-value property-tax rate per USD 10,
- ptratiopupil-teacher ratio by town
- lstatpercentage of lower status of the population
- medvmedian value of owner occupied homes in USD 1000’s

### EDA:

**Histograms**

```{r}
df %>%
  keep(is.numeric) %>% 
  gather() %>%
# Convert the data from wide to long format
# using gather(), which stacks all numeric columns on top
# of each other and creates a new column called "key" to
# keep track of the original column names.
  ggplot(aes(value))+
  geom_histogram() +
facet_wrap(~key, scales = 'free')
```

# Use facet_wrap() to display each histogram in a
# separate panel, with the key values used to split the
# data into the separate panels.
The result above is a set of histograms, one for each numeric variable in the original data
frame, with the values of that variable on the x-axis and the frequency of those values on the
y-axis. The histograms are displayed in separate panels, allowing us to easily compare the
distribution of each variable.
NOTE:-
1) **%>%** is a pipe operator in R that allows you to chain multiple functions together. It takes the
output of the previous function and passes it as the first argument of the next function. This
allows for a more concise and readable way of writing code, especially for data manipulation
and analysis.


2) In the **facet_wrap()** function, **scales = 'free'** allows each facet to have different scales
on the y-axis, meaning that the range of values shown on the y-axis can vary for each facet
based on the actual range of values in that facet, instead of having the same y-axis scale for
all facets. This is useful when the range of values in different facets varies greatly and it is
diﬀicult to see the details of each facet due to the use of a fixed scale

**Boxplot of the variables**

```{r}
df %>%
keep(is.numeric)%>%
gather() %>%
ggplot(aes(y = value))+
geom_boxplot() +
facet_wrap(~key, scales = 'free')
```
```{r}
df %>%
select(-chas) %>%
# Selects all variables from the data frame except for
# chas using select(-chas).
gather(key, val, -medv)%>%
# Convert the data from wide to long format using gather,
# with the variables key and val.
ggplot(aes(x = val,y = medv)) +
# Create a scatter plot of medv (median value of owner
# occupied homes in $1000s) on the y-axis and val on the
# x-axis using ggplot.
geom_point(alpha =0.1)+
stat_smooth(formula = y~x, method = "lm")+

facet_wrap(~key, scales = "free")
# Creates separate panels for each variable using facet_wrap(~key, scales = "free").
```

The resulting plot shows the relationship between **medv** and each of the other variables in the
data frame, with a linear regression line overlaid on each scatter plot. The transparency of the
points allows for better visualization of overlapping points, and the use of **facet_wrap** allows
for easy comparison of the relationships across the different variables. The **scales = "free"**
argument ensures that the y-axis scales are different for each panel, to avoid distortion caused
by differences in range of the **medv** variable.


**Regression Model**

We begin by creating a regression model to predictmedvusing all the predictors:

```{r}
full_model <- lm(medv ~.,df)
summary(full_model)
```
```{r}
broom::tidy(full_model)
# The code broom::tidy(full_model) is using the tidy()
# function from the broom package to extract the summary
# statistics of a fitted model object and organize them
# into a data frame. In this case, full_model is a model
# object, and tidy(full_model) returns a data frame with
# information about the coefficients of the model
# (estimate, standard error, t-statistic, p-value, etc.).
# The resulting data frame can be used for further
# analysis or visualization.
```
We can see that most of the variables are significant. However, notably ageandindusare not significant predictors of medv

Is this true?

```{r}
plot(medv~age, df)
abline(lm(medv~age),col ='red')
```

```
#model_age <- lm(medv ~ age, df)
#summary(model_age)
```
NOTE:- Since the 2 10^-16 is significant and favours the alternate hypothesis as 2 10 ^-16 is
smaller than 0.05. If the p-value is less than 0.05 you can reject the null hypothesis favouring
the alternate hypothesis which means that the p-value is significant (less than 0.05). On the
other hand, if the p-value is greater than 0.05, then we cannot reject the null hypothesis. We
observe that the p-value for age is 0.786595 which is above 0.5. Hence, the null hypothesis
cannot be rejected, the p-value ultimately favoring the null hypothesis.

```{r}
plot(medv~indus, df)
abline(lm(medv ~indus), col ="blue")
model_indus <- lm(medv~indus, df)
summary(model_indus)
R<- df %>%
  keep(is.numeric) %>%
  cor()
R
# cor () calculates the correlation between each pair of variables, and assigns the resulting correlation matrix to the object R.
```

```{r}
library(corrplot)

corrplot(R, type = "upper",order = "hclust")
```

This code uses the **corrplot** package to create a visualization of the correlation matrix **R**. The
**type** argument specifies which part of the correlation matrix to display, and **upper** displays


only the upper triangle. The **order** argument specifies the order of the variables in the plot
and **hclust** orders them according to hierarchical clustering. The resulting plot displays a
color-coded matrix with the correlations between each pair of variables, where red indicates
positive correlation, blue indicates negative correlation, and white indicates no correlation.

```{r}
new_cols<- colnames(df)[-c( 5 , 13 )]
# This code is creating a new object called new_cols that
# contains all the column names of the df data frame
# except for the 5th and 13th columns. The [-c(5,13)]
# part of the code is used to exclude the columns with
# indices 5 and 13 from the original vector of column
# names.
```
```{r}
model<- lm(medv~.,df %>% select(-c(indus, nox, dis)))
# This code fits a linear regression model with medv as
# the response variable and all other variables (except
# indus, nox, and dis) as predictor variables. The select
# (-c(indus, nox, dis)) part of the code removes the
# columns indus, nox, and dis from the dataset before
# fitting the model.
```
```{r}
summary(model)
```

**Variance Inflation Factors**

Variance inflation factor (VIF) is a measure of the extent to which the variance of the esti-
mated regression coefficient is increased due to the presence of correlation among the predictor
variables in a multiple regression model. Specifically, VIF measures how much the variance
of the estimated regression coeﬀicient of each independent variable is increased due to the
correlation with other independent variables in the model. VIF values greater than 1 indicate
that the variance of the estimated regression coeﬀicient is increased due to multicollinearity.
Typically, VIF values greater than 5 or 10 are considered to be of concern.

```{r}
library(car)
vif_model<- lm(medv~.,df)
vif(vif_model) %>%knitr::kable()
```

This code calculates the variance inflation factor (VIF) for each predictor variable in a linear
regression model with the response variable **medv**. The **car** package is loaded, and the **vif()**
function is applied to a linear regression model **vif_model** that uses all the predictor variables


in the **df** data frame. The **%>%** operator is used to pass the VIF results to the **knitr::kable()**
function, which formats the output as a table.

NOTE:- Under variance inflation. If the standard error goes up then the significance value of
the null hypothesis goes up. Anything greater than 2 is considered high variance inflation and
low otherwise.

**Stepwise Regression**

```{r}
null_model <- lm(medv ~ 1 , df)
full_model <- lm(medv ~., df)
```
**null_model** is fitting a linear regression model with only an intercept term, meaning that it
assumes the response variable **medv** is not associated with any of the predictor variables in the
data frame **df**.

**full_model** is fitting a multiple linear regression model, where the response variable **medv** is
modeled as a linear combination of all the predictor variables in the data frame **df**.

```{r}
library(caret)
forward_model <- step(null_model,direction = "forward",scope = formula(full_model))
```



```{r}
summary(forward_model)
```

The code is performing forward stepwise selection using the **step()** function from the **caret**
package. It starts with a null model **null_model** which only includes the intercept term and
progressively adds one predictor at a time, based on the **direction = "forward"** argument.
The set of candidate models considered is restricted by the **scope** argument, which specifies
the full model **full_model**. Finally, **summary(forward_model)** provides a summary of the
model selected by the forward stepwise selection.

AIC stands for Akaike Information Criterion. It is a measure of the quality of a statistical
model, relative to other models for the same data. AIC estimates the relative amount of
information lost by a given model, while taking into account the number of parameters used
by the model. The model with the lowest AIC is generally preferred, as it is expected to have
the best predictive power. AIC is the measure of the fitness of the regression model similar to
R square. A low AIC score is selected and then keep subsequently adding low AIC score values
till you reach a stage when including any of the other variables, AIC then goes up. Hence, we
stop where the AIC score is lowest, known as forward selection.


## 

```{r}
backward_model <- step(full_model,direction = "backward", score =formula(full_model))
```

```{r}
summary(backward_model)
```

The **step()** function is used for stepwise regression. The **backward_model** is obtained by
applying backward stepwise regression on the **full_model**. In backward stepwise regression,
the function starts with a model that includes all the predictor variables and removes the
predictor with the highest p-value one by one until all the remaining variables have p-values
below a certain significance level (usually 0.05).

Backward selection - U start with a Full model. Exclude the lowest AIC score variable so
that AIC stays small. And when u subsequently exclude the variables there will come a time
when the AIC value might increase hence the best stopping time is when excluding variables
we have the lowest AIC variable

```{r}
selected_model <- step(full_model,direction = "both", scope =formula(full_model))
```

```{r}
summary(selected_model)
```

In the provided code, **step()** function is used to perform a stepwise regression with both
forward and backward selection. The **full_model** is specified as the scope of the search,
meaning that all predictor variables will be considered.The resulting **selected_model** is the
selected_model selected by the stepwise regression, with the best subset of predictor variables
found by the search.

```{r}
summary(full_model)
```

```{r}
summary(selected_model)
```
